{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUkmV8caeRoQP12AXEnrHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andjelatodorovic/hypergraph_experiments/blob/main/hypergraphSBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkrfHOwTRsN9"
      },
      "outputs": [],
      "source": [
        "#TODO: optimize imports\n",
        "import gzip\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "from itertools import chain\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "import math\n",
        "from itertools import chain\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install networkx"
      ],
      "metadata": {
        "id": "oueXW8L-1yTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HSBModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_community_sizes,\n",
        "        edge_community_sizes,\n",
        "        affinity_matrix,\n",
        "        seed,\n",
        "        with_hash=False,\n",
        "        identifier=None,\n",
        "    ):\n",
        "        self.seed = seed\n",
        "        self.random_state = np.random.RandomState(seed)\n",
        "        self.node_community_sizes = node_community_sizes\n",
        "        self.edge_community_sizes = edge_community_sizes\n",
        "        self.affinity_matrix = affinity_matrix\n",
        "        self.n_node_communities = len(self.node_community_sizes)\n",
        "        self.n_edge_communities = len(self.edge_community_sizes)\n",
        "        self.node_communities = list(\n",
        "            chain.from_iterable(\n",
        "                [n] * d for n, d in enumerate(self.node_community_sizes)\n",
        "            ),\n",
        "        )\n",
        "        self.edge_communities = list(\n",
        "            chain.from_iterable(\n",
        "                [n] * d for n, d in enumerate(self.edge_community_sizes)\n",
        "            ),\n",
        "        )\n",
        "        self.M_csr = self._generate_edges()\n",
        "        self.M_csc = self.M_csr.tocsc()\n",
        "        self.n = int(self.M_csr.shape[0])\n",
        "        self.m = int(self.M_csr.shape[-1])\n",
        "        self.c = int(self.M_csr.nnz)\n",
        "        self.with_hash = with_hash\n",
        "        self.node_degree = [int(self.M_csr[i, :].nnz) for i in range(self.n)]\n",
        "        self.edge_cardinality = [int(self.M_csc[:, j].nnz) for j in range(self.m)]\n",
        "        self.node_neighborhood_size = [\n",
        "            len(set(self.M_csc[:, self.M_csr[i, :].nonzero()[-1]].nonzero()[0]))\n",
        "            for i in range(self.M_csr.shape[0])\n",
        "        ]\n",
        "        self.edge_neighborhood_size = [\n",
        "            len(set(self.M_csr[self.M_csc[:, j].nonzero()[0]].nonzero()[-1]))\n",
        "            for j in range(self.M_csc.shape[-1])\n",
        "        ]\n",
        "        self.node_labels = self._generate_node_labels()\n",
        "        self.identifier = identifier\n",
        "\n",
        "    def _generate_edges(self):\n",
        "        row_ind = list()\n",
        "        col_ind = list()\n",
        "        for node_idx, v in enumerate(self.node_communities):\n",
        "            affinities = self.affinity_matrix[v, :]\n",
        "            for comm_idx, (community_size, affinity) in enumerate(\n",
        "                zip(self.edge_community_sizes, affinities)\n",
        "            ):\n",
        "                n_edges_to_sample = int(\n",
        "                    self.random_state.choice([math.ceil, math.floor])(\n",
        "                        affinity * community_size\n",
        "                    )\n",
        "                )\n",
        "                if n_edges_to_sample > 0:\n",
        "                    edges_to_choose_from = [\n",
        "                        idx\n",
        "                        for idx, e in enumerate(self.edge_communities)\n",
        "                        if e == comm_idx\n",
        "                    ]\n",
        "                    edges_sampled = self.random_state.choice(\n",
        "                        edges_to_choose_from, size=n_edges_to_sample, replace=True\n",
        "                    )\n",
        "                    row_ind.extend([node_idx] * len(edges_sampled))\n",
        "                    col_ind.extend(edges_sampled)\n",
        "        data = [1] * len(col_ind)\n",
        "        # csr_matrix ignores duplicate entries\n",
        "        return csr_matrix((data, (row_ind, col_ind)))\n",
        "\n",
        "    def _generate_feature_matrix(self):\n",
        "        feature_matrix = np.zeros((self.n, 1))  # adjust?\n",
        "        community_params = {}\n",
        "        for community in range(self.n_node_communities):\n",
        "            mean = self.random_state.uniform(2, 20)\n",
        "            std_dev = self.random_state.uniform(1, 3)\n",
        "            community_params[community] = (mean, std_dev)\n",
        "        for node_idx in range(self.n):\n",
        "            community = self.node_communities[node_idx]\n",
        "            mean, std_dev = community_params[community]\n",
        "            #feature_matrix[node_idx, 0] = self.random_state.normal(mean, std_dev)\n",
        "            feature_matrix[node_idx] = self.random_state.normal(mean, std_dev)\n",
        "        return feature_matrix\n",
        "\n",
        "    def print_hyperedges(self):\n",
        "      for i in range(self.M_csr.shape[0]):\n",
        "        hyperedge_nodes = self.M_csr[i].indices\n",
        "        print(f\"Hyperedge {i}: {hyperedge_nodes}\")\n",
        "\n",
        "    def _generate_ihg_tsv_string(self):\n",
        "        c2r = [\n",
        "            list(map(lambda x: int(x + 1), self.M_csc[:, c].nonzero()[0]))\n",
        "            for c in range(self.M_csc.shape[-1])\n",
        "        ]\n",
        "        if self.identifier is None:\n",
        "            return \"\\n\".join([\"\\t\".join(map(str, e)) for e in c2r])\n",
        "        else:\n",
        "            return \"\\n\".join(\n",
        "                [f\"{self.identifier}\\t\" + \"\\t\".join(map(str, e)) for e in c2r]\n",
        "            )\n",
        "\n",
        "    def write_ihg_tsv_gz(self, savepath):\n",
        "        c2r_string = self._generate_ihg_tsv_string()\n",
        "        os.makedirs(savepath, exist_ok=True)\n",
        "        with gzip.open(\n",
        "            f\"{savepath}/{self._get_filename()}.ihg.tsv.gz\", \"wt\", encoding=\"UTF-8\"\n",
        "        ) as zipfile:\n",
        "            zipfile.write(c2r_string)\n",
        "\n",
        "    def _generate_ihg_features(self):\n",
        "        features = {\n",
        "            \"node_degree\": self.node_degree,\n",
        "            \"edge_cardinality\": self.edge_cardinality,\n",
        "            \"node_neighborhood_size\": self.node_neighborhood_size,\n",
        "            \"edge_neighborhood_size\": self.edge_neighborhood_size,\n",
        "            \"node_labels\": self.node_labels,\n",
        "            \"node_communities\": list(map(lambda x: x + 1, self.node_communities)),\n",
        "            \"edge_communities\": list(map(lambda x: x + 1, self.edge_communities)),\n",
        "            \"config\": dict(\n",
        "                seed=self.seed,\n",
        "                node_community_sizes=self.node_community_sizes,\n",
        "                edge_community_sizes=self.edge_community_sizes,\n",
        "                affinity_matrix=self.affinity_matrix.tolist(),\n",
        "            ),\n",
        "            \"filename\": self._get_filename(),\n",
        "        }\n",
        "        if self.identifier is not None:\n",
        "            features[\"identifier\"] = self.identifier\n",
        "        return features\n",
        "\n",
        "    def write_ihg_features_gz(self, savepath):\n",
        "        with gzip.open(\n",
        "            f\"{savepath}/{self._get_filename()}.ihg.json.gz\", \"wt\", encoding=\"UTF-8\"\n",
        "        ) as zipfile:\n",
        "            json.dump(\n",
        "                self._generate_ihg_features(),\n",
        "                zipfile,\n",
        "            )\n",
        "\n",
        "    def _get_filename(self):\n",
        "        return f\"HSBM_n-{self.n}_m-{self.m}_c-{self.c}_nnc-{self.n_node_communities}_nmc-{self.n_edge_communities}_seed-{self.seed}{'' if not self.with_hash else '_h-' + str(hash(str(self.seed) + str(self.affinity_matrix.tolist())))}\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Hypergraph Stochastic Block Model with {self.n} nodes and {self.m} edges, {self.n_node_communities} node communities and {self.n_edge_communities} edge communities>\"\n",
        "\n",
        "    '''\n",
        "    def _generate_node_labels(self, noise_prob=0.1):\n",
        "      labels = {}\n",
        "      for node_idx, v in enumerate(self.node_communities):\n",
        "          community_probabilities = self.affinity_matrix[v, :]\n",
        "\n",
        "          # Introduce noise in label selection\n",
        "          if self.random_state.rand() < noise_prob:\n",
        "              # With probability `noise_prob`, choose a random label\n",
        "              label = self.random_state.choice(range(len(community_probabilities)))\n",
        "          else:\n",
        "              # Otherwise, choose label based on community probabilities\n",
        "              label = self.random_state.choice(range(len(community_probabilities)), p=community_probabilities)\n",
        "\n",
        "          labels[node_idx] = label\n",
        "\n",
        "      return labels\n",
        "    '''\n",
        "\n",
        "    def _generate_node_labels(self, percent_label_1 = 0.05, option = 'experiment1'):\n",
        "        num_nodes = len(self.node_communities)\n",
        "        num_label_1_nodes = int(percent_label_1 * num_nodes)\n",
        "        node_indices = list(range(num_nodes))\n",
        "        label_1_nodes = self.random_state.choice(node_indices, size=num_label_1_nodes, replace=False)\n",
        "\n",
        "        labels = {}\n",
        "        for node_idx in node_indices:\n",
        "            if node_idx in label_1_nodes:\n",
        "                labels[node_idx] = 1\n",
        "            else:\n",
        "                labels[node_idx] = 0\n",
        "\n",
        "        # here i ensure no connected nodes have same label 1 - might be more difficult to propagate? adjust learning task?\n",
        "        for node_idx in label_1_nodes:\n",
        "            neighbors = np.nonzero(self.M_csr[node_idx])[1]\n",
        "\n",
        "            for neighbor in neighbors:\n",
        "                if labels[neighbor] == 1:\n",
        "                    labels[node_idx] = 0\n",
        "                    break\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def _export_hyperedges(self):\n",
        "        hyperedges_dict = {}\n",
        "        for i in range(self.M_csr.shape[0]):\n",
        "            edges = self.M_csr[i].indices\n",
        "            edges = [e for e in edges if e != i]  # exclude self-loops\n",
        "            hyperedges_dict[i] = edges\n",
        "        return hyperedges_dict\n",
        "\n",
        "    def generate_splits_and_export(self, n_splits=10, test_size=0.2, random_state=None, output_dir=\".\"):\n",
        "        ss = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
        "        for i, (train_index, test_index) in enumerate(ss.split(range(self.n))):\n",
        "            split = {'train': train_index.tolist(), 'test': test_index.tolist()}\n",
        "            output_filename = f\"{output_dir}/{i+1}.pickle\"\n",
        "            with open(output_filename, 'wb') as file:\n",
        "                pickle.dump(split, file)\n",
        "\n",
        "\n",
        "    def to_bipartite_graph(self):\n",
        "      G = nx.Graph()\n",
        "      G.add_nodes_from(range(self.n), bipartite=0)\n",
        "      G.add_nodes_from(range(self.m), bipartite=1)\n",
        "\n",
        "      for i in range(self.M_csr.shape[0]):\n",
        "          hyperedge_nodes = self.M_csr[i].indices\n",
        "          for node_idx in hyperedge_nodes:\n",
        "              if node_idx != i:  # Exclude self-loops\n",
        "                  G.add_edge(i, node_idx)\n",
        "\n",
        "      return G\n",
        "\n",
        "      def print_node_labels(self):\n",
        "        print(\"Node Labels:\")\n",
        "        for node_idx, label in self.node_labels.items():\n",
        "            print(f\"Node {node_idx}: Label {label}\")\n",
        "\n",
        "\n",
        "\n",
        "    def visualize_bipartite_graph(self):\n",
        "        G = nx.Graph()\n",
        "        hyperedge_nodes = set()\n",
        "        original_nodes = set()\n",
        "\n",
        "        for i in range(self.M_csr.shape[0]):\n",
        "            hyperedge_id = f\"{i}e\"\n",
        "            hyperedge_nodes.add(hyperedge_id)\n",
        "            for node_idx in self.M_csr[i].indices:\n",
        "                original_node_id = f\"{node_idx}n\"\n",
        "                original_nodes.add(original_node_id)\n",
        "                G.add_edge(hyperedge_id, original_node_id)\n",
        "\n",
        "        #sort for better visibility\n",
        "        hyperedge_nodes_sorted = sorted(hyperedge_nodes, key=lambda x: int(x[:-1]), reverse=True)\n",
        "        original_nodes_sorted = sorted(original_nodes, key=lambda x: int(x[:-1]), reverse=True)\n",
        "\n",
        "\n",
        "        pos = {}\n",
        "        for i, node_id in enumerate(hyperedge_nodes_sorted):\n",
        "            pos[node_id] = (0, i)\n",
        "        for j, node_id in enumerate(original_nodes_sorted):\n",
        "            pos[node_id] = (1, j)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        node_colors = ['orange' if node_id.endswith('e') else 'lightpink' for node_id in G.nodes()]\n",
        "        labels = {node_id: node_id for node_id in G.nodes()}\n",
        "        nx.draw(G, pos, with_labels=True, labels=labels, node_color=node_colors, node_size=300, font_size=8)\n",
        "        plt.title(\"Bipartite Graph Visualization\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    #vary based on the node community\n",
        "\n",
        "    def visualize_bipartite_graph(self):\n",
        "      G = nx.Graph()\n",
        "      hyperedge_nodes = set()\n",
        "      original_nodes = set()\n",
        "\n",
        "      for i in range(self.M_csr.shape[0]):\n",
        "          hyperedge_id = f\"{i}e\"\n",
        "          hyperedge_nodes.add(hyperedge_id)\n",
        "          for node_idx in self.M_csr[i].indices:\n",
        "              original_node_id = f\"{node_idx}n\"\n",
        "              original_nodes.add(original_node_id)\n",
        "              G.add_edge(hyperedge_id, original_node_id)\n",
        "\n",
        "      # Sort nodes for better visibility\n",
        "      hyperedge_nodes_sorted = sorted(hyperedge_nodes, key=lambda x: int(x[:-1]), reverse=True)\n",
        "      original_nodes_sorted = sorted(original_nodes, key=lambda x: int(x[:-1]), reverse=True)\n",
        "\n",
        "      pos = {}\n",
        "      for i, node_id in enumerate(hyperedge_nodes_sorted):\n",
        "          pos[node_id] = (0, i)\n",
        "      for j, node_id in enumerate(original_nodes_sorted):\n",
        "          pos[node_id] = (1, j)\n",
        "\n",
        "      plt.figure(figsize=(10, 8))\n",
        "\n",
        "      # Create node color mapping based on node communities\n",
        "      if len(G.original_nodes()) != len(self.node_communities):\n",
        "          print(\"Error: Number of nodes in G does not match length of node_communities\")\n",
        "          return\n",
        "\n",
        "      unique_communities = set(self.node_communities)\n",
        "      color_map = cm.get_cmap('tab10', len(unique_communities))  # Choose a colormap\n",
        "      community_color_dict = {c: color_map(i) for i, c in enumerate(unique_communities)}\n",
        "\n",
        "      # Assign node colors based on node communities\n",
        "      node_colors = []\n",
        "      for node_id in G.nodes():\n",
        "          if node_id.endswith('n'):\n",
        "              node_idx = int(node_id[:-1])\n",
        "              if node_idx < len(self.node_communities):\n",
        "                  community_label = self.node_communities[node_idx]\n",
        "                  if community_label in community_color_dict:\n",
        "                      node_colors.append(community_color_dict[community_label])\n",
        "                  else:\n",
        "                      node_colors.append('gray')  # Default color for unknown community\n",
        "              else:\n",
        "                  node_colors.append('gray')  # Default color for out-of-range index\n",
        "          else:\n",
        "              node_colors.append('gray')  # Default color for hyperedge nodes (shouldn't happen)\n",
        "\n",
        "      # Draw nodes and edges\n",
        "      nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=300, font_size=8, cmap=color_map)\n",
        "\n",
        "      plt.title(\"Bipartite Graph Visualization\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "womAGaV2Rtvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_community_sizes = [4, 4, 4]\n",
        "edge_community_sizes = [5, 5, 5]\n",
        "affinity_matrix = np.random.rand(len(node_community_sizes), len(edge_community_sizes))\n",
        "affinity_matrix = affinity_matrix / affinity_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "affinity_matrix = np.eye(3,3) * 0.2 + np.random.uniform(0,0.3) #adding noise but ensuring the sparsity is perserved\n",
        "seed = 42\n",
        "\n",
        "#change affinity matrix to be diagonally dominant\n",
        "\n",
        "model = HSBModel(node_community_sizes, edge_community_sizes, affinity_matrix, seed)\n",
        "print(model.affinity_matrix)\n",
        "model.print_hyperedges()\n",
        "model.visualize_bipartite_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "82PRbJS5yBOe",
        "outputId": "086b415c-f1cf-4e11-f91f-62e9f70257db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.45950428 0.25950428 0.25950428]\n",
            " [0.25950428 0.45950428 0.25950428]\n",
            " [0.25950428 0.25950428 0.45950428]]\n",
            "Hyperedge 0: [ 2  3  4  9 11 12]\n",
            "Hyperedge 1: [ 2  4  7 14]\n",
            "Hyperedge 2: [ 1  3  8 10 13]\n",
            "Hyperedge 3: [ 3  4  5  7 11 13]\n",
            "Hyperedge 4: [ 2  5  8 12 14]\n",
            "Hyperedge 5: [ 0  4  5  6  8 11]\n",
            "Hyperedge 6: [ 0  6  9 13]\n",
            "Hyperedge 7: [ 3  5  7  8 13]\n",
            "Hyperedge 8: [ 1  9 11 13]\n",
            "Hyperedge 9: [ 1  8 10 13]\n",
            "Hyperedge 10: [ 4  9 11 14]\n",
            "Hyperedge 11: [ 3  9 10 14]\n",
            "Error: Number of nodes in G does not match length of node_communities\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_edge_cardinalities(M_csc):\n",
        "    edge_cardinalities = [int(M_csc[:, j].nnz) for j in range(M_csc.shape[-1])]\n",
        "    return edge_cardinalities\n",
        "\n",
        "def compute_edge_neighborhood_sizes(M_csr, M_csc):\n",
        "    edge_neighborhood_sizes = [\n",
        "            len(set(M_csr[M_csc[:, j].nonzero()[0]].nonzero()[-1]))\n",
        "            for j in range(M_csc.shape[-1])\n",
        "        ]\n",
        "    return edge_neighborhood_sizes\n",
        "\n",
        "def compute_node_degrees(M_csr):\n",
        "    node_degrees = [int(M_csr[i, :].nnz) for i in range(M_csr.shape[0])]\n",
        "    return node_degrees\n",
        "\n",
        "def compute_node_neighborhood_sizes(M_csr, M_csc):\n",
        "    node_neighborhood_sizes = [\n",
        "            len(set(M_csc[:, M_csr[i, :].nonzero()[-1]].nonzero()[0]))\n",
        "            for i in range(M_csr.shape[0])\n",
        "        ]\n",
        "    return node_neighborhood_sizes"
      ],
      "metadata": {
        "id": "KMHe3-M117d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "def main():\n",
        "    # Example parameters\n",
        "    # num_nodes = 10000\n",
        "    # num_communities = 5\n",
        "\n",
        "    # # Generate node_community_sizes and edge_community_sizes\n",
        "    # node_community_sizes = np.random.randint(50, 200, size=num_communities)\n",
        "    # edge_community_sizes = np.random.randint(10, 50, size=num_communities)\n",
        "    #Generate a random affinity matrix\n",
        "    # affinity_matrix = np.random.rand(num_communities, num_communities)\n",
        "\n",
        "    node_community_sizes = [50, 50, 50]\n",
        "    edge_community_sizes = [10, 10, 10]\n",
        "    affinity_matrix = np.random.rand(len(node_community_sizes), len(edge_community_sizes))\n",
        "    affinity_matrix = affinity_matrix / affinity_matrix.sum(axis=1)[:, np.newaxis]\n",
        "    seed = 42\n",
        "\n",
        "    model = HSBModel(node_community_sizes, edge_community_sizes, affinity_matrix, seed)\n",
        "\n",
        "\n",
        "    #can this be generalized to different values for edge and node comm sizes?\n",
        "\n",
        "    affinity_matrix = affinity_matrix / affinity_matrix.sum(axis=1)[:, np.newaxis]\n",
        "    print(affinity_matrix)\n",
        "\n",
        "    #check the symmetry here for the affinity matrix? and the meaning of the affinity\n",
        "\n",
        "    seed = 42\n",
        "\n",
        "    # Create HSBModel instance\n",
        "    model = HSBModel(\n",
        "        node_community_sizes=node_community_sizes,\n",
        "        edge_community_sizes=edge_community_sizes,\n",
        "        affinity_matrix=affinity_matrix,\n",
        "        seed=seed,\n",
        "        with_hash=False,\n",
        "        identifier=None\n",
        "    )\n",
        "\n",
        "    # Generate and export splits\n",
        "    model.generate_splits_and_export(n_splits=10, test_size=0.2, output_dir=\"splits\")\n",
        "\n",
        "    # Print IHG features\n",
        "    ihg_features = model._generate_ihg_features()\n",
        "    edges = model.M_csr.nonzero()\n",
        "    labels = model._generate_node_labels()\n",
        "    features = model._generate_feature_matrix()\n",
        "    hyperedges_dict = model._export_hyperedges()\n",
        "\n",
        "    # Save hyperedges, labels, and features to pickle files\n",
        "    with open(\"hypergraph.pickle\", 'wb') as f_hyperedges:\n",
        "        pickle.dump(model._export_hyperedges(), f_hyperedges)\n",
        "\n",
        "    with open(f\"labels.pickle\", 'wb') as f_labels:\n",
        "        pickle.dump(list(model.node_labels.values()), f_labels)\n",
        "\n",
        "    with open(f\"features.pickle\", 'wb') as f_matrix:\n",
        "        pickle.dump(model._generate_feature_matrix(), f_matrix)\n",
        "\n",
        "    print(model.__repr__())\n",
        "    print(\"\\nHSBModel instance created and files printed successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhdjaKKmXMja",
        "outputId": "efb62e34-252d-437e-8b09-c2222baf947c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.03687163 0.75014885 0.21297952]\n",
            " [0.24095868 0.35649626 0.40254506]\n",
            " [0.11960581 0.15341057 0.72698362]]\n",
            "<Hypergraph Stochastic Block Model with 150 nodes and 30 edges, 3 node communities and 3 edge communities>\n",
            "\n",
            "HSBModel instance created and files printed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9QKtSrEfzY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def generate_splits_and_export(n, n_splits=10, test_size=0.2, random_state=None, output_dir=\".\"):\n",
        "    ss = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=random_state)\n",
        "    for i, (train_index, test_index) in enumerate(ss.split(range(n))):\n",
        "        split = {'train': train_index.tolist(), 'test': test_index.tolist()}\n",
        "        output_filename = f\"{output_dir}/split_{i}.pkl\"\n",
        "        with open(output_filename, 'wb') as file:\n",
        "            pickle.dump(split, file)\n",
        "        print(f\"Split {i} exported to {output_filename}\")"
      ],
      "metadata": {
        "id": "0KgJSjaU1srn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jpTMm2Ci1tlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math, numpy as np, scipy.sparse as sp\n",
        "import torch.nn as nn, torch.nn.functional as F, torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.modules.module import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class HyperGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, nlayer, V, E, X):\n",
        "        \"\"\"\n",
        "        d: initial node-feature dimension\n",
        "        h: number of hidden units\n",
        "        c: number of classes\n",
        "        \"\"\"\n",
        "        super(HyperGCN, self).__init__()\n",
        "        d, l, c = nfeat, nlayer, nclass\n",
        "        cuda = False\n",
        "        mediators = True\n",
        "        fast = False\n",
        "\n",
        "        h = [d]\n",
        "        for i in range(l-1):\n",
        "            power = l - i + 2\n",
        "            h.append(2**power)\n",
        "        h.append(c)\n",
        "\n",
        "        if fast:\n",
        "            reapproximate = False\n",
        "            structure = Laplacian(V, E, X, mediators)\n",
        "        else:\n",
        "            reapproximate = True\n",
        "            structure = E\n",
        "\n",
        "        self.layers = nn.ModuleList([HyperGraphConvolution(h[i], h[i+1], reapproximate, cuda) for i in range(l)])\n",
        "        self.do, self.l = 0.5, 3\n",
        "        self.structure, self.m = structure, mediators\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, H):\n",
        "        \"\"\"\n",
        "        an l-layer GCN\n",
        "        \"\"\"\n",
        "        do, l, m = self.do, self.l, self.m\n",
        "\n",
        "        for i, hidden in enumerate(self.layers):\n",
        "            H = F.relu(hidden(self.structure, H, m))\n",
        "            if i < l - 1:\n",
        "                V = H\n",
        "                H = F.dropout(H, do, training=self.training)\n",
        "\n",
        "        return F.log_softmax(H, dim=1)\n",
        "\n",
        "class HyperGraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, a, b, reapproximate=True, cuda=True):\n",
        "        super(HyperGraphConvolution, self).__init__()\n",
        "        self.a, self.b = a, b\n",
        "        self.reapproximate, self.cuda = reapproximate, cuda\n",
        "\n",
        "        self.W = Parameter(torch.FloatTensor(a, b))\n",
        "        self.bias = Parameter(torch.FloatTensor(b))\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1. / math.sqrt(self.W.size(1))\n",
        "        self.W.data.uniform_(-std, std)\n",
        "        self.bias.data.uniform_(-std, std)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, structure, H, m=True):\n",
        "        W, b = self.W, self.bias\n",
        "        HW = torch.mm(H, W)\n",
        "\n",
        "        if self.reapproximate:\n",
        "            n, X = H.shape[0], HW.cpu().detach().numpy()\n",
        "            A = Laplacian(n, structure, X, m)\n",
        "        else: A = structure\n",
        "\n",
        "        if self.cuda: A = A.cuda()\n",
        "        A = Variable(A)\n",
        "\n",
        "        AHW = SparseMM.apply(A, HW)\n",
        "        return AHW + b\n",
        "\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.a) + ' -> ' \\\n",
        "               + str(self.b) + ')'\n",
        "\n",
        "\n",
        "\n",
        "class SparseMM(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Sparse x dense matrix multiplication with autograd support.\n",
        "    Implementation by Soumith Chintala:\n",
        "    https://discuss.pytorch.org/t/\n",
        "    does-pytorch-support-autograd-on-sparse-matrix/6156/7\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, M1, M2):\n",
        "        ctx.save_for_backward(M1, M2)\n",
        "        return torch.mm(M1, M2)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        M1, M2 = ctx.saved_tensors\n",
        "        g1 = g2 = None\n",
        "\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            g1 = torch.mm(g, M2.t())\n",
        "\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            g2 = torch.mm(M1.t(), g)\n",
        "\n",
        "        return g1, g2\n",
        "\n",
        "\n",
        "\n",
        "def Laplacian(V, E, X, m):\n",
        "    \"\"\"\n",
        "    approximates the E defined by the E Laplacian with/without mediators\n",
        "\n",
        "    arguments:\n",
        "    V: number of vertices\n",
        "    E: dictionary of hyperedges (key: hyperedge, value: list/set of hypernodes)\n",
        "    X: features on the vertices\n",
        "    m: True gives Laplacian with mediators, while False gives without\n",
        "\n",
        "    A: adjacency matrix of the graph approximation\n",
        "    returns:\n",
        "    updated data with 'graph' as a key and its value the approximated hypergraph\n",
        "    \"\"\"\n",
        "\n",
        "    edges, weights = [], {}\n",
        "    rv = np.random.rand(X.shape[1])\n",
        "\n",
        "    for k in E.keys():\n",
        "        hyperedge = list(E[k])\n",
        "\n",
        "        p = np.dot(X[hyperedge], rv)   #projection onto a random vector rv\n",
        "        s, i = np.argmax(p), np.argmin(p)\n",
        "        Se, Ie = hyperedge[s], hyperedge[i]\n",
        "\n",
        "        # two stars with mediators\n",
        "        c = 2*len(hyperedge) - 3    # normalisation constant\n",
        "        if m:\n",
        "\n",
        "            # connect the supremum (Se) with the infimum (Ie)\n",
        "            edges.extend([[Se, Ie], [Ie, Se]])\n",
        "\n",
        "            if (Se,Ie) not in weights:\n",
        "                weights[(Se,Ie)] = 0\n",
        "            weights[(Se,Ie)] += float(1/c)\n",
        "\n",
        "            if (Ie,Se) not in weights:\n",
        "                weights[(Ie,Se)] = 0\n",
        "            weights[(Ie,Se)] += float(1/c)\n",
        "\n",
        "            # connect the supremum (Se) and the infimum (Ie) with each mediator\n",
        "            for mediator in hyperedge:\n",
        "                if mediator != Se and mediator != Ie:\n",
        "                    edges.extend([[Se,mediator], [Ie,mediator], [mediator,Se], [mediator,Ie]])\n",
        "                    weights = update(Se, Ie, mediator, weights, c)\n",
        "        else:\n",
        "            edges.extend([[Se,Ie], [Ie,Se]])\n",
        "            e = len(hyperedge)\n",
        "\n",
        "            if (Se,Ie) not in weights:\n",
        "                weights[(Se,Ie)] = 0\n",
        "            weights[(Se,Ie)] += float(1/e)\n",
        "\n",
        "            if (Ie,Se) not in weights:\n",
        "                weights[(Ie,Se)] = 0\n",
        "            weights[(Ie,Se)] += float(1/e)\n",
        "\n",
        "    return adjacency(edges, weights, V)\n",
        "\n",
        "\n",
        "\n",
        "def update(Se, Ie, mediator, weights, c):\n",
        "    \"\"\"\n",
        "    updates the weight on {Se,mediator} and {Ie,mediator}\n",
        "    \"\"\"\n",
        "\n",
        "    if (Se,mediator) not in weights:\n",
        "        weights[(Se,mediator)] = 0\n",
        "    weights[(Se,mediator)] += float(1/c)\n",
        "\n",
        "    if (Ie,mediator) not in weights:\n",
        "        weights[(Ie,mediator)] = 0\n",
        "    weights[(Ie,mediator)] += float(1/c)\n",
        "\n",
        "    if (mediator,Se) not in weights:\n",
        "        weights[(mediator,Se)] = 0\n",
        "    weights[(mediator,Se)] += float(1/c)\n",
        "\n",
        "    if (mediator,Ie) not in weights:\n",
        "        weights[(mediator,Ie)] = 0\n",
        "    weights[(mediator,Ie)] += float(1/c)\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "\n",
        "def adjacency(edges, weights, n):\n",
        "    \"\"\"\n",
        "    computes an sparse adjacency matrix\n",
        "\n",
        "    arguments:\n",
        "    edges: list of pairs\n",
        "    weights: dictionary of edge weights (key: tuple representing edge, value: weight on the edge)\n",
        "    n: number of nodes\n",
        "\n",
        "    returns: a scipy.sparse adjacency matrix with unit weight self loops for edges with the given weights\n",
        "    \"\"\"\n",
        "\n",
        "    dictionary = {tuple(item): index for index, item in enumerate(edges)}\n",
        "    edges = [list(itm) for itm in dictionary.keys()]\n",
        "    organised = []\n",
        "\n",
        "    for e in edges:\n",
        "        i,j = e[0],e[1]\n",
        "        w = weights[(i,j)]\n",
        "        organised.append(w)\n",
        "\n",
        "    edges, weights = np.array(edges), np.array(organised)\n",
        "    adj = sp.coo_matrix((weights, (edges[:, 0], edges[:, 1])), shape=(n, n), dtype=np.float32)\n",
        "    adj = adj + sp.eye(n)\n",
        "\n",
        "    A = symnormalise(sp.csr_matrix(adj, dtype=np.float32))\n",
        "    A = ssm2tst(A)\n",
        "    return A\n",
        "\n",
        "\n",
        "\n",
        "def symnormalise(M):\n",
        "    \"\"\"\n",
        "    symmetrically normalise sparse matrix\n",
        "\n",
        "    arguments:\n",
        "    M: scipy sparse matrix\n",
        "\n",
        "    returns:\n",
        "    D^{-1/2} M D^{-1/2}\n",
        "    where D is the diagonal node-degree matrix\n",
        "    \"\"\"\n",
        "\n",
        "    d = np.array(M.sum(1))\n",
        "\n",
        "    dhi = np.power(d, -1/2).flatten()\n",
        "    dhi[np.isinf(dhi)] = 0.\n",
        "    DHI = sp.diags(dhi)    # D half inverse i.e. D^{-1/2}\n",
        "\n",
        "    return (DHI.dot(M)).dot(DHI)\n",
        "\n",
        "\n",
        "\n",
        "def ssm2tst(M):\n",
        "    \"\"\"\n",
        "    converts a scipy sparse matrix (ssm) to a torch sparse tensor (tst)\n",
        "\n",
        "    arguments:\n",
        "    M: scipy sparse matrix\n",
        "\n",
        "    returns:\n",
        "    a torch sparse tensor of M\n",
        "    \"\"\"\n",
        "\n",
        "    M = M.tocoo().astype(np.float32)\n",
        "\n",
        "    indices = torch.from_numpy(np.vstack((M.row, M.col))).long()\n",
        "    values = torch.from_numpy(M.data)\n",
        "    shape = torch.Size(M.shape)\n",
        "\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "9z8WUwqwIa2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "--BeTtod2UX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}